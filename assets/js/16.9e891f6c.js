(window.webpackJsonp=window.webpackJsonp||[]).push([[16],{388:function(a,h,s){"use strict";s.r(h);var t=s(2),e=Object(t.a)({},(function(){var a=this,h=a._self._c;return h("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[h("h1",{attrs:{id:"hashmap、hashtable、concurrenthashmap"}},[h("a",{staticClass:"header-anchor",attrs:{href:"#hashmap、hashtable、concurrenthashmap"}},[a._v("#")]),a._v(" hashMap、hashTable、ConcurrentHashMap")]),a._v(" "),h("h2",{attrs:{id:"hashmap"}},[h("a",{staticClass:"header-anchor",attrs:{href:"#hashmap"}},[a._v("#")]),a._v(" hashMap")]),a._v(" "),h("ul",[h("li",[a._v("jdk1.7之前数据结构是数组 + 链表，主体是数组，链表是为了解决哈希冲突，链表是单链表，")]),a._v(" "),h("li",[a._v("jdk1.8之后引入红黑树结构，用来解决哈希冲突之后链表过长，查询效率变低的问题")]),a._v(" "),h("li",[a._v("默认负载因子是0.75，默认map长度是16，扩容阈值是 负载因子乘与map长度，默认是0.75*16 = 12")]),a._v(" "),h("li",[a._v("当链表长度大于8，实际数组总量大于64，链表才会变成红黑树，如果没达到这个要求，会触发扩容机制，如果达到要求转化成红黑树之后，数据减少红黑树也会重新转化成链表")]),a._v(" "),h("li",[a._v("之所以转化红黑树还需要这么多的条件，主要是链表长度小的时候其实查询效率还是可以的，之后链表长度大于8，实际数据总量大于64，查询效率才会不理想")]),a._v(" "),h("li",[a._v("map的扩容机制也是默认按两倍进行扩容，扩容后如果是未指定map初始容量一般是16变成32，相当于一个比bit位的运算，\n如果是一个bit位，原map中数据索引的值变为 本身 + 扩容长度，如果没有达到一位，则索引值不变")]),a._v(" "),h("li",[a._v("hashMap线程不安全")]),a._v(" "),h("li",[a._v("解决哈希冲突的方法有再哈希法、开放地址法和链地址法等方法，hashMap使用的是链地址法，解决哈希冲入计算，开发地址法是，如果p=H(key)出现冲突时，则以p为基础，再次hash。\n再哈希法是：提供多个不同的hash函数，R1=H1(key1)发生冲突时，再计算R2=H2（key1）")])]),a._v(" "),h("h2",{attrs:{id:"hashtable"}},[h("a",{staticClass:"header-anchor",attrs:{href:"#hashtable"}},[a._v("#")]),a._v(" hashTable")]),a._v(" "),h("ul",[h("li",[a._v("同样和hashMap，都是基于哈希表实现，同样是数组 + 链表的数据结构，都实现了序列化接口Serializable支持序列化")]),a._v(" "),h("li",[a._v("hashTable是线程安全的")])]),a._v(" "),h("h2",{attrs:{id:"concurrenthashmap"}},[h("a",{staticClass:"header-anchor",attrs:{href:"#concurrenthashmap"}},[a._v("#")]),a._v(" concurrentHashMap")]),a._v(" "),h("ul",[h("li",[a._v("jdk1.7 中的concurrentHashMap是有segment数组和HashEntry数组结构组成，相当于把hashMap的桶拆分成多个小数组segment,每个segment由多个hashEntry组成。")]),a._v(" "),h("li",[a._v("jdk1.7 中的concurrentHashMap通过给segment数组加锁的设定，在线程占用锁访问其中一段数据时，其他数据不受影响，做到了真正的并发访问。也就是segment分段锁")]),a._v(" "),h("li",[a._v("jdk1.8 之后的concurrentHashMap和HashMap一样采用了数组 + 链表 + 红黑树的结构，抛弃了原有的segment分段锁，采用CAS + synchronized实现了更细粒度的锁")]),a._v(" "),h("li",[a._v("jdk1.8 之后的concurrentHashMap将锁的级别控制在更细粒度的哈希桶数组元素级别，也就是说只需要锁住这个桶就不会影响其他的桶元素读写大大提高了并发")]),a._v(" "),h("li",[a._v("concurrentHashMap的key和value不能为null，会影响数据判断")]),a._v(" "),h("li",[a._v("相较于同样线程安全的hashTable,ConcurrentHashMap 的效率要高于 Hashtable，因为Hashtable的锁是锁全表，ConcurrentHashMap是更细粒度的锁，可以提高并发支持")])]),a._v(" "),h("h2",{attrs:{id:"hashmap、hashtable区别"}},[h("a",{staticClass:"header-anchor",attrs:{href:"#hashmap、hashtable区别"}},[a._v("#")]),a._v(" hashMap、hashTable区别")]),a._v(" "),h("ol",[h("li",[a._v("继承的父类不同\nHashMap继承自AbstractMap类。但二者都实现了Map接口。\nHashtable继承自Dictionary类，Dictionary类是一个已经被废弃的类（见其源码中的注释）。父类都被废弃，自然而然也没人用它的子类Hashtable了。")]),a._v(" "),h("li",[a._v("是否允许null\nHashmap是允许key和value为null值的，用containsValue和containsKey方法判断是否包含对应键值对；HashTable键值对都不能为空，否则包空指针异常。")]),a._v(" "),h("li",[a._v("计算hash值方式不同")]),a._v(" "),h("li",[a._v("扩容方式不同（容量不够）")]),a._v(" "),h("li",[a._v("解决hash冲突方式不同（地址冲突）")]),a._v(" "),h("li",[a._v("包含的contains方法不同")]),a._v(" "),h("li",[a._v("HashMap线程不安全,HashTable线程安全")])]),a._v(" "),h("h2",{attrs:{id:"concurrenthashmap和hashtable区别"}},[h("a",{staticClass:"header-anchor",attrs:{href:"#concurrenthashmap和hashtable区别"}},[a._v("#")]),a._v(" ConcurrentHashMap和Hashtable区别")]),a._v(" "),h("ol",[h("li",[a._v("主要区别就是围绕着锁的粒度以及如何锁。Hashtable的实现方式---锁整个hash表；而右边则是ConcurrentHashMap的实现方式---锁桶(或段)\n只有写入操作的时候才进行锁处理，读的时候基本是不进行锁处理的，这里再次体现了ConcurrentHashMap并发效率")])])])}),[],!1,null,null,null);h.default=e.exports}}]);